决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，
每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，
测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。


决策树算法本质上就是要找出每一列的最佳划分以及不同列划分的先后顺序及排布。

主要包括三个步骤：特征选择，决策树的生成，决策树的修剪。



一、特征选择：依据信息增益或信息增益比

不同算法使用的依据不同

（一）信息增益与信息增益比



1、信息增益




熵：表示随机变量的不确定性。

条件熵：在一个条件下，随机变量的不确定性。

信息增益：熵 - 条件熵

在一个条件下，信息不确定性减少的程度！

通俗地讲，X(明天下雨)是一个随机变量，X的熵可以算出来， Y(明天阴天)也是随机变量，在阴天情况下下雨的信息熵我们如果也知道的话（此处需要知道其联合概率分布或是通过数据估计）即是条件熵。

2、信息增益比




3、联系
在离散值且分布项数有限情况下两者区分度不大，当区分度过细（门牌号，身份证号甚至是连续数据如身高）时需要防止区分度太细


（二）



二、决策树的生成

（一）ID3算法

以信息增益度量属性选择，选择分裂后信息增益最大的属性进行分裂

只有树的生成，没有剪枝，容易发生过拟合

（二）C4.5算法

改进了ID3算法，使用信息增益比

